{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d006b2ea-9dfe-49c7-88a9-a5a0775185fd",
   "metadata": {},
   "source": [
    "# Additional End of week Exercise - week 2\n",
    "\n",
    "Now use everything you've learned from Week 2 to build a full prototype for the technical question/answerer you built in Week 1 Exercise.\n",
    "\n",
    "This should include a Gradio UI, streaming, use of the system prompt to add expertise, and the ability to switch between models. Bonus points if you can demonstrate use of a tool!\n",
    "\n",
    "If you feel bold, see if you can add audio input so you can talk to it, and have it respond with audio. ChatGPT or Claude can help you, or email me if you have questions.\n",
    "\n",
    "I will publish a full solution here soon - unless someone beats me to it...\n",
    "\n",
    "There are so many commercial applications for this, from a language tutor, to a company onboarding solution, to a companion AI to a course (like this one!) I can't wait to see your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5ef8f-1c9d-46e1-b6df-1544e71bd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import ollama\n",
    "\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b67670-fc2b-4744-bce6-cbd801f67019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15c866-a6c5-4e4d-bfee-981a2e3759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0186e-5ac0-4283-b699-f5b584dd13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an assistant designed to analyze the user's technical question carefully and provide a well-structured, detailed, and clearly explained answer. \n",
    "\n",
    "Your response should include:\n",
    "- A concise introduction to the problem.\n",
    "- A step-by-step explanation or solution.\n",
    "- Relevant code examples (if applicable).\n",
    "- Clear and logical reasoning.\n",
    "- Use Markdown formatting for proper structure, including headers, code blocks, lists, and emphasis (bold, italics) as needed to enhance readability.\n",
    "\n",
    "Always ensure your answer is clear, accurate, and easy to follow for someone with basic to intermediate technical knowledge.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e7793-b8f5-44f4-aded-5562f633271a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import openai\n",
    "\n",
    "# OpenAI API call function\n",
    "def openai_api_call(history):\n",
    "    # Prepare the message sequence for OpenAI\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history  # History already in the correct format\n",
    "    \n",
    "    # Call the OpenAI API\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # Append the assistant's reply to the history (still in correct format)\n",
    "    history += [{\"role\": \"assistant\", \"content\": reply}]\n",
    "    \n",
    "    return history\n",
    "\n",
    "# LLaMA API call function\n",
    "def llama_api_call(history):\n",
    "    # Prepare the message sequence for LLaMA\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history  # History already in the correct format\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Concatenate the chunks of response\n",
    "    response_text = \"\"\n",
    "    for chunk in response:\n",
    "        response_text += chunk['message']['content'] or ''\n",
    "        response_text = response_text.replace(\"```\",\"\")\n",
    "    \n",
    "    # Append the assistant's reply to the history (still in correct format)\n",
    "    history += [{\"role\": \"assistant\", \"content\": response_text}]\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Chat function for Gradio UI\n",
    "def chat_with_ai(history, model_choice):\n",
    "    if model_choice == \"OpenAI\":\n",
    "        # Use OpenAI API call\n",
    "        response = openai_api_call(history)\n",
    "    else:\n",
    "        # Use LLaMA API call\n",
    "        response = llama_api_call(history)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def do_entry(message, history):\n",
    "    # Append the user input to the history as a dictionary\n",
    "    history += [{\"role\": \"user\", \"content\": message}]  # User's message as dictionary\n",
    "    return \"\", history\n",
    "\n",
    "# Gradio UI setup\n",
    "with gr.Blocks() as ui:\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"AI Assistant\", height=500, type=\"messages\")\n",
    "    with gr.Row():\n",
    "        model_selector = gr.Radio([\"OpenAI\", \"LLaMA\"], label=\"Choose AI Model\", value=\"OpenAI\")\n",
    "    with gr.Row():\n",
    "        user_input = gr.Textbox(label=\"Chat here:\")\n",
    "    with gr.Row():\n",
    "        clear = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # Handling user input submission\n",
    "    user_input.submit(do_entry, inputs=[user_input, chatbot], outputs=[user_input, chatbot]).then(\n",
    "        chat_with_ai, inputs=[chatbot, model_selector], outputs=[chatbot]\n",
    "    )\n",
    "    \n",
    "    # Clear chat button handler\n",
    "    clear.click(lambda: None, inputs=None, outputs=chatbot, queue=False)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78738fae-a39f-4aa7-8627-a85316bd48a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
